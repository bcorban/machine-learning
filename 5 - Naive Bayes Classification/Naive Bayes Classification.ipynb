{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Bayesian models for Machine Learning<br>Naive Bayes Classification</div>\n",
    "\n",
    "One very common application of naive Bayes classifiers is document classification (e-mail spam filtering, sentiment analysis on social networks, technical documentation classification, customer appreciations, etc.). \n",
    "\n",
    "Naive Bayes classifiers for documents estimate the probability of a given document belonging to a certain class Y of documents, based on the document's contents Xi.\n",
    "\n",
    "\n",
    "Suppose we want to predict the probability that sample $x$ has label $y$. This is a probability estimation problem that can be written:\n",
    "$$\\mathbb{P}(Y=y|X=x)$$\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "$$\\mathbb{P}(Y=y|X=x) =\\frac{\\mathbb{P}(X=x|Y=y)\\cdot\\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}$$\n",
    "$$\\textrm{posterior} = \\frac{\\textrm{likelihood}\\cdot\\textrm{prior}}{\\textrm{evidence}}$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Bayesian inference** is the problem of estimating this **posterior distribution**.<br>\n",
    "In plain words, it consists in estimating the probability of label $y$, given an input $x$, using previously seen data to estimate the **likelihood** of an $x$ input associated to label $y$ and the general **prior** probability of observing label $y$.\n",
    "</div>\n",
    "\n",
    "Note that Bayesian inference applies both to classification and regression.\n",
    "\n",
    "The goal of Bayesian inference is to estimate the label distribution for a given $x$ and use them to predict the correct label, so it is a *probabilistic approach to Machine Learning*.\n",
    "\n",
    "The Bayesian predictor (classifier or regressor) returns the label that maximizes the posterior probability distribution.\n",
    "\n",
    "In this (first) notebook on Bayesian modeling in ML, we will explore the method of Naive Bayes Classification.\n",
    "\n",
    "1. [The naive Bayes assumption](#sec1)\n",
    "2. [Naive Bayes classifiers in scikit-learn](#sec2)\n",
    "3. [Examples](#sec3)\n",
    "    1. [The \"spam or ham?\" example](#sec3-1)\n",
    "    2. [The NIST example](#sec3-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id=\"sec1\"></a>The naive Bayes assumption\n",
    "\n",
    "Let's start with some illustrative data. We consider an artificial data set of 9 individuals. The first column in our data set is the sex ($S=0$ for male, 1 for female), the second is the height $H$ (in meters), the third is the weight $W$ (in kilos) and the last is the foot size $F$ (in centimeters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.82, 82.  , 30.  ],\n",
       "       [ 0.  ,  1.8 , 86.  , 28.  ],\n",
       "       [ 0.  ,  1.7 , 77.  , 30.  ],\n",
       "       [ 0.  ,  1.8 , 75.  , 25.  ],\n",
       "       [ 1.  ,  1.52, 45.  , 15.  ],\n",
       "       [ 1.  ,  1.65, 68.  , 20.  ],\n",
       "       [ 1.  ,  1.68, 59.  , 18.  ],\n",
       "       [ 1.  ,  1.75, 68.  , 23.  ],\n",
       "       [ 1.  ,  1.58, 49.  , 19.  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig_size=(10, 10)\n",
    "\n",
    "data = np.loadtxt(\"sex_classif.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "- Using matplotlib, bokeh, seaborn or plotly, plot one relevant figure on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fbff10bf670>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your plot here\n",
    "import pandas as pd\n",
    "\n",
    "plt.scatter(data[:,1],data[:,2],c=data[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to answer the question: is $(H=1.81, W=59, F=21)$ male or female?\n",
    "\n",
    "Let's try to estimate $\\mathbb{P}(S=0|H=1.81, W=59, F=21)$.\n",
    "\n",
    "According to Bayes' theorem, the probability that a person that measures 1.81m, weights 59kgs and has a foot size of 21cm is male, is actually the likelihood of observing a person with such features among males, multiplied by the probability of observing males in the population, divided by the probability of observing an individual with these features.\n",
    "\n",
    "That's a long sentence. Let's write that mathematically:\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81, W=59, F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "Let's make that more readable and more general:\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H,W,F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H,W,F)}$$\n",
    "\n",
    "Interestingly, since our goal is only to compare the probabilities for $S=0$ and $S=1$, the denominator in the last equation won't be relevant. So we are left with two terms to estimate, given the available data:\n",
    "- $\\mathbb{P}(S=0)$: the prior - the probability that any individual is $S=0$, regardless of his/her physical attributes;\n",
    "- $\\mathbb{P}(H=1.81, W=59, F=21 | S=0)$: the likelihood of meeting somebody with the specified features, given that his/her sex is $S=0$.\n",
    "\n",
    "The prior, in this case, is easy to estimate by comparing the frequencies of male and female individuals in the population.\n",
    "\\begin{gather*}\n",
    "\\mathbb{P}(S=0) = \\frac{4}{9}\\\\\n",
    "\\mathbb{P}(S=1) = \\frac{5}{9}\n",
    "\\end{gather*}\n",
    "Technically, the estimate above is obtained by [*maximum likelihood estimation*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "The likelihood, however, is a bit trickier. Can we directly estimate the **joint probability** of the 3 variables $(H,W,F)$?\n",
    "\n",
    "Theoretically, we can. We can assume that among male individuals, $(H,W,F)$ are distributed according to a multivariate Normal distribution, with mean $\\mu=(\\mu_H, \\mu_W, \\mu_F)$ and covariance matrix $\\Sigma$. The trick is then to estimate $\\mu$ and $\\Sigma$.\n",
    "\n",
    "As a matter of fact, estimating $\\mu$ and $\\Sigma$ without further hypothesis would require quite a lot of data, especially because $\\Sigma$ captures the **correlation** between $H$, $W$ and $F$.\n",
    "\n",
    "$\\Sigma$ is a $3\\times 3$ matrix, so it involves 9 parameters to estimate, and we unfortunately only have 9 data points.\n",
    "\n",
    "Let's rephrase this from another perspective. With some basic probabilities, we have:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S, H) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S, H, W)\n",
    "\\end{align*}\n",
    "\n",
    "Those three probabilities are univariate probabilities, much easier to estimate. However, the first one is a function of $S$ only, the second one depends on $S$ and $H$ and the third one depends on $S$, $H$ and $W$. To get an accurate estimate of the third one, we would need samples of the distribution of $F$ in enough points in the space of $(S,H,W)$ to cover it reasonably. This would require a number of data points that is exponential in the number of variables. That's what is called the **curse of dimensionality**, which makes this estimation problem difficult.\n",
    "\n",
    "Let's make this concrete. Suppose we discretize $H$, $W$ and $F$ in 10 bins each and suppose we require 100 samples to get a correct estimate of $\\mathbb{P}(F | S, H, W)$ for any given value of $(F, S, H, W)$. Then we need $100\\cdot 10^3\\cdot 2$ samples to correctly estimate this probability for all possible values of $(F, S, H, W)$. More generally, if we had $n$ continuous features rather than just three, we would require a number of data points that is exponential in $n$.\n",
    "\n",
    "To circumvent this problem, we are going to make a very **naive** assumption (hence the name of the method). We are going to assume that the weight, the height and the foot size are totally independent variables, that is the probability that a person be 1.85m is the same whatever his/her weight and foot size.\n",
    "\n",
    "Obviously, this hypothesis is very strong and clearly does not hold is most real-world cases. But we will assume it nonetheless. In this case, the likelihood estimation becomes:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
    "& \\cdot \\mathbb{P}(W | S) \\\\\n",
    "& \\cdot \\mathbb{P}(F | S)\n",
    "\\end{align*}\n",
    "\n",
    "Each of these probabilities now only depends on the label $S$ and is much easier to estimate from the data. This **conditional independence** assumption is called the **naive Bayes hypothesis**. It allow us to give a (very bad) estimate of $\\mathbb{P}(X | Y)$ and hence of $\\mathbb{P}(Y|X)$.\n",
    "\n",
    "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H | S)\\cdot \\mathbb{P}(W | S) \\cdot \\mathbb{P}(F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H, W, F)}$$\n",
    "\n",
    "Or, in our case:\n",
    "\n",
    "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81 | S=0)\\cdot \\mathbb{P}(W=59 | S=0) \\cdot \\mathbb{P}(F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
    "\n",
    "The **naive Bayes classifier** is then the classifier that estimates all class probabilities and returns the one with maximum probability.\n",
    "\n",
    "$$f(H, W, F) = \\arg\\max_{s} \\mathbb{P}(S=s|H,W,F) = \\arg\\max_{s} \\mathbb{P}(H|S=s)\\cdot \\mathbb{P}(W|S=s) \\cdot \\mathbb{P}(F|S=s)\\cdot \\mathbb{P}(S=s)$$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**<br>\n",
    "Let's implement a naive Bayes classifier on the data above, just to practice. We will assume that the $\\mathbb{P}(X | S)$ distributions are Gaussians (for $X = H,W,$ or $F$). Compute the scores and probabilities for each sex, for $(H=1.81, W=59, F=21)$.<br>\n",
    "Hint: use the `np.mean` and `np.std` functions to estimate distribution parameters. Use `scipy.stats.norm.pdf` to compute the Gaussian probability density function in a given input.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score male    : 6.981284895980458e-10\n",
      "score female  : 0.0012161942837264688\n",
      "proba male    : 5.740267802562792e-07\n",
      "proba female  : 0.9999994259732197\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "# Estimate distribution parameters for males\n",
    "dataM = data[data[:,0]==0]\n",
    "mu_HS0 = np.mean(dataM[:,1])\n",
    "std_HS0 = np.std(dataM[:,1])\n",
    "mu_WS0 = np.mean(dataM[:,2])\n",
    "std_WS0 = np.std(dataM[:,2])\n",
    "mu_FS0 = np.mean(dataM[:,3])\n",
    "std_FS0 = np.std(dataM[:,3])\n",
    "pS0 = dataM.shape[0]/data.shape[0]\n",
    "\n",
    "# Estimate distribution parameters for females\n",
    "dataF = data[data[:,0]==1]\n",
    "mu_HS1 = np.mean(dataF[:,1])\n",
    "std_HS1 = np.std(dataF[:,1])\n",
    "mu_WS1 = np.mean(dataF[:,2])\n",
    "std_WS1 = np.std(dataF[:,2])\n",
    "mu_FS1 = np.mean(dataF[:,3])\n",
    "std_FS1 = np.std(dataF[:,3])\n",
    "pS1 = dataF.shape[0]/data.shape[0]\n",
    "\n",
    "# score that (H=1.81,W=59,F=21) is male/female\n",
    "H=1.81\n",
    "W=59\n",
    "F=21\n",
    "from scipy.stats import norm\n",
    "score_M = pS0 * norm.pdf(H,mu_HS0,std_HS0) * norm.pdf(W,mu_WS0,std_WS0) * norm.pdf(F,mu_FS0,std_FS0)\n",
    "score_F = pS1 * norm.pdf(H,mu_HS1,std_HS1) * norm.pdf(W,mu_WS1,std_WS1) * norm.pdf(F,mu_FS1,std_FS1)\n",
    "print(\"score male    :\", score_M)\n",
    "print(\"score female  :\", score_F)\n",
    "print(\"proba male    :\", score_M/(score_M+score_F))\n",
    "print(\"proba female  :\", score_F/(score_M+score_F))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we will always multiply together values that are smaller than one. The result will quickly become very small. It is a good habit to move to log-scale.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**<br>\n",
    "Reuse your code above to compute log scores instead of scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "log_score_M = np.log(pS0) + norm.logpdf(H,mu_HS0,std_HS0) + norm.logpdf(W,mu_WS0,std_WS0) + norm.logpdf(F,mu_FS0,std_FS0)\n",
    "log_score_F = np.log(pS1) + norm.logpdf(H,mu_HS1,std_HS1) + norm.logpdf(W,mu_WS1,std_WS1) + norm.logpdf(F,mu_FS1,std_FS1)\n",
    "print(\"log score male:    \", log_score_M)\n",
    "print(\"log score female:  \", log_score_F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: $(H=1.81,W=59,F=21)$ is most probably female.\n",
    "\n",
    "Let's generalize.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "    \n",
    "Given $n$ features $X_i$ and classes $Y$, **naive Bayes classifiers** estimate (from data) the distributions $\\mathbb{P}(Y)$ and $\\mathbb{P}(X_i|Y)$. Then, using Bayes rule and the naive Bayes assumption, they predict the most probable estimated class:\n",
    "\\begin{align*}\n",
    "\\arg\\max_{y} \\mathbb{P}(Y=y|X=x) & = \\arg\\max_{y} \\frac{\\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}\\\\\n",
    "& = \\arg\\max_{y} \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)\\\\\n",
    "& = \\arg\\max_{y} \\sum\\limits_{i=1}^n \\log\\left(\\mathbb{P}(X_i=x_i|Y=y)\\right) + \\log\\left(\\mathbb{P}(Y=y)\\right)\n",
    "\\end{align*}\n",
    "</div>\n",
    "\n",
    "Note that although it is not compulsory to compute the denominator, it is quite straightforward since:\n",
    "\\begin{align*}\n",
    "\\mathbb{P}(X=x) &= \\sum\\limits_y \\mathbb{P}(X=x|Y=y)\\mathbb{P}(Y=y)\\\\\n",
    "&= \\sum\\limits_y \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y) \n",
    "\\end{align*}\n",
    "So it's the sum of the numerator's values for all $y$, so it's just a matter of normalizing the scores obtained.\n",
    "\n",
    "A really nice thing about naive Bayes classifiers is that it is an **online method**, since most univariate probability distributions can be updated incrementally, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. Naive Bayes classifiers in scikit-learn\n",
    "\n",
    "Once again, scikit-learn has a [naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) implementation. It allows three kind of distributions for the $X_i|Y$ variables: Normal (continuous), Bernouilli or Multinomial (discrete).\n",
    "Let's directly use it on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "gnb.fit(X,y)\n",
    "xtest = np.array([[1.81,59,21]])\n",
    "print(\"Prediction: \", gnb.predict(xtest))\n",
    "print(\"Probas:     \", gnb.predict_proba(xtest))\n",
    "print(\"Log probas: \",gnb.predict_log_proba(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. Examples\n",
    "\n",
    "## <a id=\"sec3-1\"></a> 3.1 The \"spam or ham?\" example\n",
    "\n",
    "Let's scale up and apply naive Bayes classification on the ling-spam data. We will assume a multinomial distribution of word $i$ appearing in and email of class $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "print(\"data loaded\")\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:**\n",
    "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the data above. Estimate its generalization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9888017917133258\n",
      "******************** done!\n",
      "Average generalization score: 0.9291713325867861\n",
      "Standard deviation: 0.009566934162184048\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "print(\"Score:\", spam_nbc.score(Xtest,ytest))\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000)\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our model in the Tf-Idf data. Let's see how the model behaves on raw word counts.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**\n",
    "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the raw word counts data below. Estimate its generalization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9204927211646137\n",
      "******************** done!\n",
      "Average generalization score: 0.9880179171332587\n",
      "Standard deviation: 0.0032086335457770228\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/code4.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain)\n",
    "print(\"Score:\", spam_nbc.score(Xtest,ytest))\n",
    "\n",
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = spam_data.shuffle_and_split(2000, feat='wordcount')\n",
    "    spam_nbc = MultinomialNB()\n",
    "    spam_nbc.fit(Xtrain,ytrain);\n",
    "    score += [spam_nbc.score(Xtest,ytest)]\n",
    "    print('*', end='')\n",
    "print(\" done!\")\n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify which are the misclassified emails (and find the confusion matrix by the way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
    "spam_nbc = MultinomialNB()\n",
    "spam_nbc.fit(Xtrain,ytrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified messages indices: [176, 178, 188, 200, 202, 604, 605, 680, 794, 825]\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified examples\n",
    "ypredict = spam_nbc.predict(Xtest)\n",
    "misclass = np.not_equal(ypredict, ytest)\n",
    "Xmisclass = Xtest[misclass,:]\n",
    "ymisclass = ytest[misclass]\n",
    "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
    "print(\"Misclassified messages indices:\", misclass_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[729  10]\n",
      " [  0 154]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ True]\n",
      "email file: ../data/lingspam_public/bare/part1/5-1218msg1.txt\n",
      "email is a spam: False\n",
      "Subject: translators needed for women for women !\n",
      "\n",
      "i am posting the following message for my friends who are not on the list : * would you like to use your language skills to help women survivors of the war in bosnia and croatia ? women for women , a u . s . based , nonprofit sponsorship program sending letters and money each month to the region , is desperately seeking volunteer translators . we translate letters both from and to english . even if you can manage only a handful of letters each month , it would lighten the load for the few translators we have now . for more information , call our office at ( 703 ) 519-1730 , and leave a message for zainab or robin . thank you ! * you may also send an e-mail message to me mima @ seur . voa . gov and i will forward it to * women for women : .\n",
      "\n",
      "Bag of words representation (40 words in dictionary):\n",
      "{'subject': 1, 'posting': 1, 'language': 1, 'based': 1, 'following': 1, 'send': 1, 'program': 1, 'would': 2, 'like': 1, 'use': 1, 'help': 1, 'also': 1, 'office': 1, 'information': 1, 'may': 1, 'call': 1, 'mail': 1, 'thank': 1, 'even': 1, 'message': 3, 'list': 1, 'leave': 1, 'sending': 1, 'month': 2, 'money': 1, 'war': 1, 'manage': 1, 'robin': 1, 'translate': 1, 'region': 1, 'forward': 1, 'seeking': 1, 'load': 1, 'handful': 1, 'volunteer': 1, 'nonprofit': 1, 'desperately': 1, 'sponsorship': 1, 'lighten': 1, 'mima': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.31614247, 0.68385753]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some misclassified mails\n",
    "index = misclass_indices[1]+2000\n",
    "print(\"Prediction:\", spam_nbc.predict(spam_data.word_count[index,:]))\n",
    "spam_data.print_email(index)\n",
    "spam_nbc.predict_proba(spam_data.tfidf[index,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** :\n",
    "- What are the next steps to improve the results ? To create the moderation system ?\n",
    "- What questions should you ask to the tech company, before working on their data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec3-2\"></a> 3.2 The NIST example\n",
    "\n",
    "We will assume Gaussian distributions for the NIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 8, 8)\n",
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print(digits.data.shape)\n",
    "print(digits.images.shape)\n",
    "print(digits.target.shape)\n",
    "print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "#Xtrain = X[:1000,:]\n",
    "#ytrain = y[:1000]\n",
    "#Xtest = X[1000:,:]\n",
    "#ytest = y[1000:]\n",
    "\n",
    "#print(digits.DESCR)\n",
    "\n",
    "#plt.gray();\n",
    "#plt.matshow(digits.images[0]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[15]);\n",
    "#plt.show();\n",
    "#plt.matshow(digits.images[42]);\n",
    "#plt.show();\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64)\n",
      "(1000,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5 - Naive Bayes Classification/Naive Bayes Classification.ipynb Cellule 29\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(Xtrain\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(ytrain\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m digits_nbc \u001b[39m=\u001b[39m GaussianNB()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m digits_nbc\u001b[39m.\u001b[39mfit(Xtrain,ytrain)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m prediction \u001b[39m=\u001b[39m digits_nbc\u001b[39m.\u001b[39mpredict(Xtest)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "Xtrain, ytrain, Xtest, ytest  = shuffle_and_split(X,y,1000)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_nbc = GaussianNB()\n",
    "digits_nbc.fit(Xtrain,ytrain)\n",
    "prediction = digits_nbc.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_nbc.score(Xtest,ytest))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(ytest, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5 - Naive Bayes Classification/Naive Bayes Classification.ipynb Cellule 30\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_trials):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     Xtrain, ytrain, Xtest, ytest \u001b[39m=\u001b[39m shuffle_and_split(X,y,\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     digits_nbc \u001b[39m=\u001b[39m GaussianNB()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     digits_nbc\u001b[39m.\u001b[39mfit(Xtrain,ytrain)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/baptiste/Desktop/Supaero/4A/machine-learning-main/5%20-%20Naive%20Bayes%20Classification/Naive%20Bayes%20Classification.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [digits_nbc\u001b[39m.\u001b[39mscore(Xtest,ytest)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute cross-validation score\n",
    "nb_trials = 20\n",
    "score = []\n",
    "for i in range(nb_trials):\n",
    "    Xtrain, ytrain, Xtest, ytest = shuffle_and_split(X,y,1000)\n",
    "    digits_nbc = GaussianNB()\n",
    "    digits_nbc.fit(Xtrain,ytrain)\n",
    "    score += [digits_nbc.score(Xtest,ytest)]\n",
    "    print('*',end='')\n",
    "print(\" done!\")\n",
    "    \n",
    "print(\"Average generalization score:\", np.mean(score))\n",
    "print(\"Standard deviation:\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers reach their limits on data with high correlations between features (like images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
