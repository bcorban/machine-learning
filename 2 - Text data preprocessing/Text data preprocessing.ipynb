{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers. The general purpose of this notebook is to give a practical notion (through this example) of how important data pre-processing can be in a Machine Learning workflow, and generalize it to other situations.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img id=\"fig1\" src=\"https://imgs.xkcd.com/comics/constructive.png\"> \n",
    "\n",
    "A tech company comes to you to create a moderation system for their social network : they want to detect spam comments, and later on also detect offensive contents to remove them automatically.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "To showcase our proposed system, we load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\n",
    "**Questions** :\n",
    "- What simple statistics could you print on the dataset ?\n",
    "- Why is there multiple folders on the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone git@github.com:SupaeroDataScience/machine-learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part3/6-232msg1.txt\n",
      "email is a spam: False\n",
      "Subject: foreign language in commercials\n",
      "\n",
      "content - length : 1937 greetings ! i ' m wondering if someone out there can identify the languages used in two recent ibm commercials . they have out a series of three ads with people in different countries talking about ibm equipment and services . one ad has two old gentlemen walking along what looks like the seine , speaking french . a second ad has two men sitting in what is apparently a middle eastern marketplace , and a third ad has nuns discussing ibm equipment on their way to mass . i assume the men are speaking arabic ( though i would be grateful to have that confirmed ) , but i have no idea what the nuns are speaking . anyone know ? i would also like to take this opportunity to thank all those who responded to my questions early last year about some french , italian and swedish expressions in some print ads . i meant to send individual thanks , but i lost the file in which i had the respondents ! please forgive me , and accept this general acknowledgment instead . if any of you who did respond would like to know more about what we did with your input , i would be glad to send you more information ( even the paper we wrote , if you want ) . thanks very much , mary ellen renryder @ idbsu . idbsu . edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = '../data/lingspam_public/bare/'\n",
    "# train_dir = 'machine-learning/data/ling-spam/train-mails/'\n",
    "\n",
    "email_path = []\n",
    "email_label = []\n",
    "for d in os.listdir(train_dir):\n",
    "    folder = os.path.join(train_dir,d)\n",
    "    email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "    email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/baptiste/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/baptiste/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: click in /Users/baptiste/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/baptiste/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/baptiste/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/baptiste/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/baptiste/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/baptiste/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/baptiste/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run only of nltk is not installed\n",
    "# %pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: job posting - apple-iss research center\n",
      "\n",
      "content - length : 3386 apple-iss research center a us $ 10 million joint venture between apple computer inc . and the institute of systems science of the national university of singapore , located in singapore , is looking for : a senior speech scientist - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise in computational linguistics , including natural language processing and * * english * * and * * chinese * * statistical language modeling . knowledge of state-of - the-art corpus-based n - gram language models , cache language models , and part-of - speech language models are required . a text - to - speech project leader - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - the successful candidate will have research expertise expertise in two or more of the following areas : computational linguistics , including natural language parsing , lexical database design , and statistical language modeling ; text tokenization and normalization ; prosodic analysis . substantial knowledge of the phonology , syntax , and semantics of chinese is required . knowledge of acoustic phonetics and / or speech signal processing is desirable . both candidates will have a phd with at least 2 to 4 years of relevant work experience , or a technical msc degree with at least 5 to 7 years of experienc e . very strong software engineering skills , including design and implementation , and productization are required in these positions . knowledge of c , c + + and unix are preferred . a unix & c programmer - - - - - - - - - - - - - - - - - - - - we are looking for an experienced unix & c programmer , preferably with good industry experience , to join us in breaking new frontiers . strong knowledge of unix tools ( compilers , linkers , make , x - windows , e - mac , . . . ) and experience in matlab required . sun and silicon graphic experience is an advantage . programmers with less than two years industry experience need not apply . these positions include interaction with scientists in the national university of singapore , and with apple 's speech research and productization efforts located in cupertino , california . attendance and publication in international scientific / engineering conferences is encouraged . benefits include an internationally competitive salary , housing subsidy , and relocation expenses . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ send a complete resume , enclosing personal particulars , qualifications , experience and contact telephone number to : mr jean - luc lebrun center manager apple - iss research center , institute of systems science heng mui keng terrace , singapore 0511 tel : ( 65 ) 772-6571 fax : ( 65 ) 776-4005 email : jllebrun @ iss . nus . sg\n",
      "\n",
      "Bag of words representation (104 words in dict):\n",
      "{'subject': 1, 'job': 1, 'posting': 1, 'apple': 5, 'research': 6, 'center': 4, 'content': 1, 'length': 1, 'u': 2, 'million': 1, 'joint': 1, 'venture': 1, 'computer': 1, 'institute': 2, 'science': 2, 'national': 2, 'university': 2, 'looking': 2, 'senior': 1, 'speech': 5, 'scientist': 1, 'successful': 2, 'candidate': 2, 'computational': 2, 'linguistics': 2, 'natural': 2, 'language': 7, 'statistical': 2, 'modeling': 2, 'knowledge': 5, 'state': 1, 'art': 1, 'corpus': 1, 'based': 1, 'gram': 1, 'cache': 1, 'part': 1, 'text': 2, 'project': 1, 'leader': 1, 'two': 2, 'following': 1, 'lexical': 1, 'design': 2, 'normalization': 1, 'prosodic': 1, 'analysis': 1, 'substantial': 1, 'phonology': 1, 'syntax': 1, 'semantics': 1, 'acoustic': 1, 'phonetics': 1, 'signal': 1, 'desirable': 1, 'least': 2, 'relevant': 1, 'work': 1, 'experience': 6, 'technical': 1, 'degree': 1, 'strong': 2, 'engineering': 2, 'implementation': 1, 'preferred': 1, 'programmer': 2, 'experienced': 1, 'preferably': 1, 'good': 1, 'industry': 2, 'join': 1, 'breaking': 1, 'new': 1, 'make': 1, 'mac': 1, 'sun': 1, 'silicon': 1, 'graphic': 1, 'advantage': 1, 'le': 1, 'need': 1, 'apply': 1, 'include': 2, 'interaction': 1, 'attendance': 1, 'publication': 1, 'international': 1, 'scientific': 1, 'internationally': 1, 'competitive': 1, 'salary': 1, 'housing': 1, 'subsidy': 1, 'relocation': 1, 'send': 1, 'complete': 1, 'resume': 1, 'personal': 1, 'contact': 1, 'telephone': 1, 'number': 1, 'jean': 1, 'manager': 1, 'terrace': 1}\n",
      "\n",
      "Vector reprensentation (104 non-zero elements):\n",
      "  (0, 12153)\t1\n",
      "  (0, 6803)\t1\n",
      "  (0, 9514)\t1\n",
      "  (0, 651)\t5\n",
      "  (0, 10559)\t6\n",
      "  (0, 1925)\t4\n",
      "  (0, 2680)\t1\n",
      "  (0, 7148)\t1\n",
      "  (0, 13190)\t2\n",
      "  (0, 7859)\t1\n",
      "  (0, 6810)\t1\n",
      "  (0, 13708)\t1\n",
      "  (0, 2475)\t1\n",
      "  (0, 6485)\t2\n",
      "  (0, 11085)\t2\n",
      "  (0, 8203)\t2\n",
      "  (0, 13427)\t2\n",
      "  (0, 7371)\t2\n",
      "  (0, 11223)\t1\n",
      "  (0, 11809)\t5\n",
      "  (0, 11088)\t1\n",
      "  (0, 12220)\t2\n",
      "  (0, 1762)\t2\n",
      "  (0, 2473)\t2\n",
      "  (0, 7261)\t2\n",
      "  :\t:\n",
      "  (0, 7083)\t1\n",
      "  (0, 8251)\t1\n",
      "  (0, 660)\t1\n",
      "  (0, 6244)\t2\n",
      "  (0, 6545)\t1\n",
      "  (0, 892)\t1\n",
      "  (0, 9936)\t1\n",
      "  (0, 6589)\t1\n",
      "  (0, 11086)\t1\n",
      "  (0, 6591)\t1\n",
      "  (0, 2418)\t1\n",
      "  (0, 10948)\t1\n",
      "  (0, 5966)\t1\n",
      "  (0, 12189)\t1\n",
      "  (0, 10449)\t1\n",
      "  (0, 11220)\t1\n",
      "  (0, 2430)\t1\n",
      "  (0, 10637)\t1\n",
      "  (0, 9123)\t1\n",
      "  (0, 2672)\t1\n",
      "  (0, 12614)\t1\n",
      "  (0, 8468)\t1\n",
      "  (0, 6783)\t1\n",
      "  (0, 7524)\t1\n",
      "  (0, 12675)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** : \n",
    "- What is a bag-of-word representation ?\n",
    "- What kind of feature selection or feature engineering could you derivate from this bag of word representation ?\n",
    "\n",
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 14165)\t0.032361054728660896\n",
      "  (0, 13708)\t0.09341693865792444\n",
      "  (0, 13427)\t0.04632580594782004\n",
      "  (0, 13190)\t0.0646855010835856\n",
      "  (0, 13170)\t0.06453955150732485\n",
      "  (0, 12697)\t0.08043490079178701\n",
      "  (0, 12675)\t0.09688311821450302\n",
      "  (0, 12614)\t0.04784703679429355\n",
      "  (0, 12592)\t0.05338311365610194\n",
      "  (0, 12454)\t0.039309122502719764\n",
      "  (0, 12273)\t0.06949296915830819\n",
      "  (0, 12220)\t0.11254563747658812\n",
      "  (0, 12192)\t0.06328370564732295\n",
      "  (0, 12189)\t0.10856922212065918\n",
      "  (0, 12153)\t0.013792202780750298\n",
      "  (0, 12098)\t0.12125232820910133\n",
      "  (0, 11957)\t0.12798230632961755\n",
      "  (0, 11947)\t0.03803598251386359\n",
      "  (0, 11809)\t0.21043460653761\n",
      "  (0, 11462)\t0.10460145264101772\n",
      "  (0, 11443)\t0.07355117963599689\n",
      "  (0, 11223)\t0.07250316659847815\n",
      "  (0, 11220)\t0.032269775753662425\n",
      "  (0, 11207)\t0.042693269620621185\n",
      "  (0, 11088)\t0.09064924773395626\n",
      "  :\t:\n",
      "  (0, 4463)\t0.07635925771263297\n",
      "  (0, 4462)\t0.27515014309884445\n",
      "  (0, 4154)\t0.12041212907676126\n",
      "  (0, 3359)\t0.07121924460660588\n",
      "  (0, 3352)\t0.11704082832892901\n",
      "  (0, 3224)\t0.0566939874260321\n",
      "  (0, 2810)\t0.04865857690233059\n",
      "  (0, 2680)\t0.041829802224850526\n",
      "  (0, 2672)\t0.03449629337471354\n",
      "  (0, 2475)\t0.040779126685580196\n",
      "  (0, 2473)\t0.09220842802028169\n",
      "  (0, 2430)\t0.0489603876932764\n",
      "  (0, 2418)\t0.08637152806934063\n",
      "  (0, 1925)\t0.20426455053345025\n",
      "  (0, 1762)\t0.13898593831661638\n",
      "  (0, 1710)\t0.10460145264101772\n",
      "  (0, 1551)\t0.08108922126276814\n",
      "  (0, 1126)\t0.03730920077483468\n",
      "  (0, 892)\t0.07468542416318447\n",
      "  (0, 765)\t0.057128423429438274\n",
      "  (0, 660)\t0.057128423429438274\n",
      "  (0, 651)\t0.4086541737588413\n",
      "  (0, 507)\t0.038174877975532186\n",
      "  (0, 234)\t0.06228624441757873\n",
      "  (0, 121)\t0.07355117963599689\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part3/6-232msg1.txt\n",
      "email is a spam: False\n",
      "Subject: foreign language in commercials\n",
      "\n",
      "content - length : 1937 greetings ! i ' m wondering if someone out there can identify the languages used in two recent ibm commercials . they have out a series of three ads with people in different countries talking about ibm equipment and services . one ad has two old gentlemen walking along what looks like the seine , speaking french . a second ad has two men sitting in what is apparently a middle eastern marketplace , and a third ad has nuns discussing ibm equipment on their way to mass . i assume the men are speaking arabic ( though i would be grateful to have that confirmed ) , but i have no idea what the nuns are speaking . anyone know ? i would also like to take this opportunity to thank all those who responded to my questions early last year about some french , italian and swedish expressions in some print ads . i meant to send individual thanks , but i lost the file in which i had the respondents ! please forgive me , and accept this general acknowledgment instead . if any of you who did respond would like to know more about what we did with your input , i would be glad to send you more information ( even the paper we wrote , if you want ) . thanks very much , mary ellen renryder @ idbsu . idbsu . edu\n",
      "\n",
      "Bag of words representation (71 words in dictionary):\n",
      "{'subject': 1, 'content': 1, 'length': 1, 'language': 1, 'two': 3, 'send': 2, 'early': 1, 'along': 1, 'one': 1, 'would': 4, 'like': 3, 'recent': 1, 'also': 1, 'people': 1, 'different': 1, 'much': 1, 'anyone': 1, 'information': 1, 'talking': 1, 'meant': 1, 'know': 2, 'paper': 1, 'year': 1, 'eastern': 1, 'series': 1, 'please': 1, 'foreign': 1, 'wondering': 1, 'someone': 1, 'identify': 1, 'used': 1, 'three': 1, 'equipment': 2, 'ad': 3, 'old': 1, 'walking': 1, 'seine': 1, 'speaking': 3, 'second': 1, 'men': 2, 'sitting': 1, 'apparently': 1, 'middle': 1, 'third': 1, 'way': 1, 'mass': 1, 'assume': 1, 'though': 1, 'grateful': 1, 'confirmed': 1, 'idea': 1, 'take': 1, 'opportunity': 1, 'thank': 1, 'last': 1, 'print': 1, 'individual': 1, 'thanks': 2, 'lost': 1, 'file': 1, 'forgive': 1, 'accept': 1, 'general': 1, 'instead': 1, 'respond': 1, 'input': 1, 'glad': 1, 'even': 1, 'wrote': 1, 'want': 1, 'mary': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
